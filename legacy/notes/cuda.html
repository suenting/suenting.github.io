
<html><!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>suenting.github.io</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/stylesheet.css" rel="stylesheet">
    <link href="../css/style.css" rel="stylesheet">
    <link href="../css/notes.css" rel="stylesheet">
     <script src="../prettify/run_prettify.js"></script>
     
     <style>
     .anchor {
		padding-top: 73px;
		margin-top: -73px;
	}
     </style>
  </head>
  <body>
  
	<!-- header -->
	<div id="" class="navbar navbar-default navbar-fixed-top">
	<header class="inner">
          <a id="forkme_banner" href="https://github.com/suenting">View on GitHub</a>
          <a href="http://suenting.github.io"><h1 id="project_title">suenting.github.io</h1></a>
	</header>
	</div>
	
	<!-- content -->
	<div class="main-content container">
		<div class="btn-group-vertical floatmenu visible-lg">
			<a class="btn btn-default btn-lg" href="#device">CUDA</a>
			<a class="btn btn-default btn-lg" href="#kernal">Kernal Configuration</a>
			<a class="btn btn-default btn-lg" href="#memory">Memory</a>
			<a class="btn btn-default btn-lg" href="#uvm">Unified Virtual Memory</a>
		</div>
		
<h1>CUDA NOTES</h1>		
<div class="notes">
<div class="anchor" id="device"></div>
<h3 class="notesHeader">Device Query</h3>
		The below code demonstrates how to retrieve information regarding CUDA enabled cards on a machine
<pre><code class="prettyprint">
#include &lt;cuda.h&gt;
cudaGetDeviceCount(&amp;deviceCount);// returns int number of cuda enabled gpus
	
cudaDeviceProp deviceProp;
cudaGetDeviceProperties(&amp;deviceProp, deviceIndex);
deviceProp.name // GPU Name
deviceProp.major + "." + deviceProp.minor // Computational Capabilities
deviceProp.totalGlobalMem // global memory limit
deviceProp.totalConstMem // const memory limit
deviceProp.sharedMemPerBlock // shared memory limit
deviceProp.maxThreadsDim[0] x deviceProp.maxThreadsDim[1] x deviceProp.maxThreadsDim[2] // max block dimensions
deviceProp.maxGridSize[0] x deviceProp.maxGridSize[1] x deviceProp.maxGridSize[2] // max grid dimensions
deviceProp.warpSize // warp size
</code></pre>
		Additionally most cuda functions return cudaError_t which is used for error checking. 
<pre><code class="prettyprint">
cudaError_t err = cudaMalloc(...);
cudaError_t err = cudaGetLastError();
if(cudaSuccess != err)
{
	// handle error
}
</code></pre>
<div class="anchor" id="kernal"></div>
<h3 class="notesHeader">Kernal Configuration</h3>
		CUDA is designed to execute "kernal" functions across several cores on a GPU. CUDA threads are grouped into blocks, which are grouped into grids. This is defined as per below.
		Note: use cudaGetDeviceProperties to detect the maximum number of threads / block size / grid size available.
<pre><code class="prettyprint">
dim3 dimGrid(x,y,z); // number of blocks in each dimension of the grid 
dim3 dimBlock(x,y,z);// number of threads in each dimension of the block 
f&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(args); // launch kernal
cudaThreadSynchronize(); // wait for all threads to finish
</code></pre>
		Kernal functions fall into three types.
		<table style="font-size:12px;">
		<tbody>
		<tr><td>__device__</td><td>runs on the device, involked by another kernal</td></tr>
		<tr><td>__global__</td><td>runs on the device, involked by another host</td></tr>
		<tr><td>__host__</td><td>runs on the host, involked by another host</td></tr>
		</tbody>
		</table>
		Threads are assigned to Streaming Multiprocessors, with up to 8 blocks per SM. Streaming Multiprocessors manage thread/block idx values which can be used to as an unique identifier (index value)
<pre><code class="prettyprint">
// example on a kernal
int x = threadIdx.x+blockDim.x*blockIdx.x;
int y = threadIdx.y+blockDim.y*blockIdx.y;
int z = threadIdx.z+blockDim.z*blockIdx.z;
</code></pre>
		Each block can contain 32 thread warps, where a thread warp manages the control logic for each thread (line pointer). if the branch path for two threads
		on the same warp is different then the warp will execute one path then the other, resulting in a performance hit.
<div class="anchor" id="memory"></div>
<h3 class="notesHeader">Memory</h3>
		

		A GPU can only access memory stored on its chip. In order for a GPU to preform calculations and store results, memory must be allocated on the GPU, 
		and memory must be copied to the GPU if it needs to be accessed, as well as copied back to main memory for the CPU to access the results.
<pre><code class="prettyprint">
cudaMalloc(&amp;ptr1,sizeBytesAllocated); // allocate gpu memory at the address of pointer ptr1
cudaMemcpy(destinationDevice,sourceHost,inputSizeBytes,cudaMemcpyHostToDevice); // copy memory from host to device (GPU Memory)
cudaMemcpy(destinationHost, sourceDevice, inputSizeBytes, cudaMemcpyDeviceToHost);// copy memory from deivce to host (RAM)
cudaFree(ptr1);// free gpu (global) memory at ptr1
</code></pre>
		There are three types of memory.
		<table style="font-size:12px;">
		<tbody>
		<tr><td>global</td><td>accessible by gpu and can by memcpy to and from cpu</td></tr>
		<tr><td>__shared__</td><td>shared per block, declared in kernal and only accessible to given block. ( configurable size, split with L1 cache )</td></tr>
		<tr><td>const [type] __restrict__</td><td>const memory, which cannot be modified</td></tr>
		</tbody>
		</table>
		If multiple threads are sharing the work of caching data from global to shared memory, the threads must be synced to ensure that they wait untill all caching is done before proceeding.
<pre><code class="prettyprint">
__syncthreads();
</code></pre>
<div class="anchor" id="uvm"></div>
<h3 class="notesHeader">Unified Virtual Memory</h3>
With the introduction of the Maxwell architecture (CUDA version 6) and Unified Virtual Memory, it is no longer necessary to use 
cudaMemcpy to manually transfer data between Main Memory and GPU Memory. Instead by declaring memory with cudaMallocManaged, the device will
automatically copy memory from Main Memory to GPU memory and vice versa by detecting hardware pagefault access to memory from either host or device.

<pre><code class="prettyprint">
float *pData;
cudaMallocManaged(&amp;pData, SIZE); // allocate data as managed
// copy data to managed memory
fGPU&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(pData, args); // able to invoke gpu kernal with access to memory
fCPU(pData,args); // cpu also able to access memory from said pointer.
cudaFree(pData);
</code></pre> 
Note: that memory is still copied to and from the host/device when either or requires access so performance is still hit, however this is usefull to simplify application code.
</div>
	
	</div>



    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="../js/bootstrap.min.js"></script>
        <script>
    $('a').click(function(){
    $('html, body').animate({
        scrollTop: $( $(this).attr('href') ).offset().top
    }, 500);
    return false;
    });
    </script>
  </body>
</html>